{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667c264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61653fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.mean((X - X_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474dadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kl_loss =  -0.5 * numpy.sum(1 + numpy.log(Z_sigma ** 2) - numpy.square(Z_mean) - numpy.exp(np.log(Z_sigma ** 2), axis = 1)\n",
    " \n",
    "kl_loss =  -0.5 * numpy.sum(1 + numpy.log(Z_sigma ** 2) - numpy.square(Z_mean) - Z_sigma ** 2, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19bbc246",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from librosa import display\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization, \\\n",
    "    Flatten, Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25832e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).\\\n",
    "shuffle(60000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                          \n",
    "def encoder(input_encoder):\n",
    "   \n",
    "    inputs = keras.Input(shape=input_encoder, name='input_layer')\n",
    " \n",
    "    # Block-1\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides= 1, padding='same', name='conv_1')(inputs)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "     \n",
    "    # Block-2\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "     \n",
    "    # Block-3\n",
    "    x = layers.Conv2D(64, 3, 2, padding='same', name='conv_3')(x)\n",
    "    x = layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "   \n",
    "    # Block-4\n",
    "    x = layers.Conv2D(64, 3, 1, padding='same', name='conv_4')(x)\n",
    "    x = layers.BatchNormalization(name='bn_4')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_4')(x)\n",
    "    \n",
    "    # Final Block\n",
    "    flatten = layers.Flatten()(x)\n",
    "    mean = layers.Dense(2, name='mean')(flatten)\n",
    "    log_var = layers.Dense(2, name='log_var')(flatten)\n",
    "    model = tf.keras.Model(inputs, (mean, log_var), name=\"Encoder\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(input_1,input_2):\n",
    "    mean = keras.Input(shape=input_1, name='input_layer1')\n",
    "    log_var = keras.Input(shape=input_2, name='input_layer2')\n",
    "    out = layers.Lambda(sampling_reparameterization_model, name='encoder_output')([mean, log_var])\n",
    "    enc_2 = tf.keras.Model([mean,log_var], out,  name=\"Encoder_2\")\n",
    "    return enc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69913be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sampling_reparameterization(distribution_params):\n",
    "    mean, log_var = distribution_params\n",
    "    epsilon = K.random_normal(shape=K.shape(mean), mean=0., stddev=1.)\n",
    "    z = mean + K.exp(log_var / 2) * epsilon\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_decoder):\n",
    "     \n",
    "    inputs = keras.Input(shape=input_decoder, name='input_layer')\n",
    "    x = layers.Dense(3136, name='dense_1')(inputs)\n",
    "    x = layers.Reshape((7, 7, 64), name='Reshape_Layer')(x)\n",
    "    \n",
    "    # Block-1\n",
    "    x = layers.Conv2DTranspose(64, 3, strides= 1, padding='same',name='conv_transpose_1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "   \n",
    "    # Block-2\n",
    "    x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "     \n",
    "    # Block-3\n",
    "    x = layers.Conv2DTranspose(32, 3, 2, padding='same', name='conv_transpose_3')(x)\n",
    "    x = layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "     \n",
    "    # Block-4\n",
    "    outputs = layers.Conv2DTranspose(1, 3, 1,padding='same', activation='sigmoid', name='conv_transpose_4')(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr = 0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fdbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    r_loss = K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "    return 1000 * r_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf1856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mean, log_var):\n",
    "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean) - K.exp(log_var), axis = 1)\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff38d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred, mean, var):\n",
    "    r_loss = mse_loss(y_true, y_pred)\n",
    "    kl_loss = kl_loss(mean, log_var)\n",
    "    return  r_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    " \n",
    "    with tf.GradientTape() as encoder, tf.GradientTape() as decoder:\n",
    "       \n",
    "        mean, log_var = enc(images, training=True)\n",
    "        latent = sampling([mean, log_var])\n",
    "        generated_images = dec(latent, training=True)\n",
    "        loss = vae_loss(images, generated_images, mean, log_var)\n",
    " \n",
    "         \n",
    "    gradients_of_enc = encoder.gradient(loss, enc.trainable_variables)\n",
    "    gradients_of_dec = decoder.gradient(loss, dec.trainable_variables)\n",
    "     \n",
    "     \n",
    "    optimizer.apply_gradients(zip(gradients_of_enc, enc.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(gradients_of_dec, dec.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4beb561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    " \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    \n",
    "    \n",
    "train(train_dataset, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01127d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 15\n",
    " \n",
    "m, v = enc.predict(x_test[:25])\n",
    "latent = sampling([m,v])\n",
    "reconst = dec.predict(latent)\n",
    " \n",
    "fig = plt.figure(figsize=(figsize, 10))\n",
    " \n",
    "for i in range(25):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.15, str(label_dict[y_test[i]]), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "     \n",
    "    ax.imshow(reconst[i, :,:,0]*255, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96934a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'cartoonset100k',\n",
    "  image_size=(256, 256),\n",
    "  batch_size=batch_size,\n",
    "  label_mode=None)\n",
    " \n",
    "normalization_layer = layers.experimental.preprocessing.Rescaling(scale= 1./255)\n",
    " \n",
    "normalized_ds = train_ds.map(lambda x: normalization_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32964e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_encoder):\n",
    "     \n",
    "     \n",
    "    inputs = keras.Input(shape=input_encoder, name='input_layer')\n",
    "     \n",
    "    # Block-1\n",
    "    x = layers.Conv2D(32, kernel_size=3, strides= 2, padding='same', name='conv_1')(inputs)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "   \n",
    "    # Block-2\n",
    "    x = layers.Conv2D(64, kernel_size=3, strides= 2, padding='same', name='conv_2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "    \n",
    "    # Block-3\n",
    "    x = layers.Conv2D(64, 3, 2, padding='same', name='conv_3')(x)\n",
    "    x = layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "    \n",
    "    # Block-4\n",
    "    x = layers.Conv2D(64, 3, 2, padding='same', name='conv_4')(x)\n",
    "    x = layers.BatchNormalization(name='bn_4')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_4')(x)\n",
    "   \n",
    "    # Block-5\n",
    "    x = layers.Conv2D(64, 3, 2, padding='same', name='conv_5')(x)\n",
    "    x = layers.BatchNormalization(name='bn_5')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_5')(x)\n",
    "  \n",
    " \n",
    "    # Final Block\n",
    "    flatten = layers.Flatten()(x)\n",
    "    mean = layers.Dense(200, name='mean')(flatten)\n",
    "    log_var = layers.Dense(200, name='log_var')(flatten)\n",
    "    model = tf.keras.Model(inputs, (mean, log_var), name=\"Encoder\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390643cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "                            \n",
    "def decoder(input_decoder):\n",
    "     \n",
    "    inputs = keras.Input(shape=input_decoder, name='input_layer')\n",
    "    x = layers.Dense(4096, name='dense_1')(inputs)\n",
    "    x = layers.Reshape((8,8,64), name='Reshape')(x)\n",
    "     \n",
    "    # Block-1\n",
    "    x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same',name='conv_transpose_1')(x)\n",
    "    x = layers.BatchNormalization(name='bn_1')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_1')(x)\n",
    "    \n",
    "    # Block-2\n",
    "    x = layers.Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(x)\n",
    "    x = layers.BatchNormalization(name='bn_2')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_2')(x)\n",
    "  \n",
    "    # Block-3\n",
    "    x = layers.Conv2DTranspose(64, 3, 2, padding='same', name='conv_transpose_3')(x)\n",
    "    x = layers.BatchNormalization(name='bn_3')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_3')(x)\n",
    "    \n",
    "    # Block-4\n",
    "    x = layers.Conv2DTranspose(32, 3, 2, padding='same', name='conv_transpose_4')(x)\n",
    "    x = layers.BatchNormalization(name='bn_4')(x)\n",
    "    x = layers.LeakyReLU(name='lrelu_4')(x)\n",
    " \n",
    "   \n",
    "    # Block-5\n",
    "    outputs = layers.Conv2DTranspose(3, 3, 2,padding='same', activation='sigmoid', name='conv_transpose_5')(x)\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"Decoder\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 15\n",
    " \n",
    " \n",
    "fig = plt.figure(figsize=(figsize, 10))\n",
    " \n",
    "for i in range(25):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    pred = reconstruction[i, :, :, :] * 255\n",
    "    pred = np.array(pred)  \n",
    "    pred = pred.astype(np.uint8)\n",
    "     \n",
    "    ax.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4acfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_show = 5000\n",
    "figsize = 12\n",
    " \n",
    "example_idx = np.random.choice(range(len(x_test)), n_to_show)\n",
    "example_images = x_test[example_idx]\n",
    " \n",
    "m, v = enc.predict(example_images)\n",
    "embeddings = sampling([m,v])\n",
    " \n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "plt.scatter(embeddings[:, 0] , embeddings[:, 1], alpha=0.5, s=2)\n",
    "plt.xlabel(\"Dimension-1\", size=20)\n",
    "plt.ylabel(\"Dimension-2\", size=20)\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.title(\"Projection of 2D Latent-Space (Fashion-MNIST)\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c32ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 15\n",
    " \n",
    "min_x = min(embeddings[:, 0])\n",
    "max_x = max(embeddings[:, 0])\n",
    "min_y = min(embeddings[:, 1])\n",
    "max_y = max(embeddings[:, 1])\n",
    " \n",
    " \n",
    "x = np.random.uniform(min_x,max_x, size = 10)\n",
    "y = np.random.uniform(min_y,max_y, size = 10)\n",
    "z_grid = np.array(list(zip(x, y)))\n",
    "reconst = dec.predict(z_grid)\n",
    " \n",
    " \n",
    "fig = plt.figure(figsize=(figsize, 10))\n",
    " \n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.15, str(np.round(z_grid[i],1)), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "     \n",
    "    ax.imshow(reconst[i, :,:,0]*255, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 15\n",
    " \n",
    "x = np.random.normal(size = (10,2))\n",
    "reconstruct = dec.predict(x)\n",
    " \n",
    "fig = plt.figure(figsize=(figsize, 10))\n",
    " \n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    ax.imshow(reconstruct[i, :,:,0]*255, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdcfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 15\n",
    " \n",
    " \n",
    "min_x = lat_space.min(axis=0)\n",
    "max_x = lat_space.max(axis=0)\n",
    "x = np.random.uniform(size = (10,200))\n",
    "x = x * (max_x - (np.abs(min_x))) \n",
    "print(x.shape)\n",
    "reconstruct = dec.predict(x)\n",
    " \n",
    " \n",
    "fig = plt.figure(figsize=(figsize, 10))\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    " \n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    pred = reconstruct[i, :, :, :] * 255\n",
    "    pred = np.array(pred)  \n",
    "    pred = pred.astype(np.uint8)\n",
    "    ax.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a95485",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = 15\n",
    " \n",
    "x = np.random.normal(size = (10,200))\n",
    "reconstruct = dec.predict(x)\n",
    " \n",
    " \n",
    "fig = plt.figure(figsize=(figsize, 10))\n",
    " \n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.axis('off')\n",
    "    pred = reconstruct[i, :, :, :] * 255\n",
    "    pred = np.array(pred)  \n",
    "    pred = pred.astype(np.uint8)\n",
    "    ax.imshow(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
